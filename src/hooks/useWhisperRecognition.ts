import { useState, useEffect, useRef, useCallback } from 'react';
import { AudioRecorder, transcribeAudio, OPENAI_API_KEY } from '../lib/whisper';

export type RecognitionState = 'idle' | 'starting' | 'listening' | 'processing' | 'stopping';

export interface UseWhisperRecognitionOptions {
  apiKey?: string;
  intervalMs?: number; // éŸ³å£°ã‚’é€ä¿¡ã™ã‚‹é–“éš”ï¼ˆãƒŸãƒªç§’ï¼‰
}

export function useWhisperRecognition(options: UseWhisperRecognitionOptions = {}) {
  const {
    apiKey = OPENAI_API_KEY,
    intervalMs = 3000, // 3ç§’ã”ã¨ã«é€ä¿¡ï¼ˆçŸ­ã™ãã‚‹ã¨èªè­˜ç²¾åº¦ãŒä¸‹ãŒã‚‹ï¼‰
  } = options;

  const [transcript, setTranscript] = useState<string>('');
  const [interimTranscript, setInterimTranscript] = useState<string>('');
  const [state, setState] = useState<RecognitionState>('idle');
  const [error, setError] = useState<string | null>(null);
  const [isSupported, setIsSupported] = useState<boolean>(false);
  const [isSpeechDetected, setIsSpeechDetected] = useState<boolean>(false);
  const [audioLevel, setAudioLevel] = useState<number>(0);
  const [currentGain, setCurrentGain] = useState<number>(5.0);
  const [processingStatus, setProcessingStatus] = useState<string>('');

  const recorderRef = useRef<AudioRecorder | null>(null);
  const intervalRef = useRef<ReturnType<typeof setInterval> | null>(null);
  const flushIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);
  const isProcessingRef = useRef<boolean>(false);
  const pendingTextRef = useRef<string>('');
  const apiKeyRef = useRef<string>(apiKey);

  // APIã‚­ãƒ¼ã‚’refã§ä¿æŒï¼ˆå†ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚’é˜²ãï¼‰
  useEffect(() => {
    apiKeyRef.current = apiKey;
  }, [apiKey]);

  // ã‚µãƒãƒ¼ãƒˆç¢ºèª
  useEffect(() => {
    const supported = typeof navigator.mediaDevices !== 'undefined' && 
      typeof navigator.mediaDevices.getUserMedia === 'function' && 
      typeof window.MediaRecorder !== 'undefined';
    setIsSupported(supported);
    if (!supported) {
      setError('ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯éŸ³å£°éŒ²éŸ³ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚');
    }
  }, []);

  // ã‚²ã‚¤ãƒ³å€¤ã®å¤‰æ›´ï¼ˆéŒ²éŸ³ä¸­ã§ã‚‚ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã«åæ˜ ï¼‰
  const setGain = useCallback((value: number) => {
    setCurrentGain(value);
    if (recorderRef.current) {
      recorderRef.current.setGain(value);
    }
  }, []);

  // å®šæœŸçš„ã«éŸ³å£°ã‚’é€ä¿¡ã—ã¦æ–‡å­—èµ·ã“ã—
  const processAudio = useCallback(async () => {
    if (!recorderRef.current) {
      console.log('[Whisper] No recorder');
      return;
    }
    if (isProcessingRef.current) {
      console.log('[Whisper] Already processing');
      return;
    }
    if (!recorderRef.current.isRecording()) {
      console.log('[Whisper] Not recording');
      return;
    }

    const blob = recorderRef.current.getIntermediateBlob();
    console.log('[Whisper] Got blob:', blob?.size || 0, 'bytes');
    
    // æœ€å°ã‚µã‚¤ã‚ºã‚’100ãƒã‚¤ãƒˆã«ä¸‹ã’ã‚‹
    if (!blob || blob.size < 100) {
      setProcessingStatus('éŸ³å£°ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...');
      return;
    }

    isProcessingRef.current = true;
    setProcessingStatus('Whisper APIã«é€ä¿¡ä¸­...');
    
    // å‡¦ç†ä¸­ã¯ã€Œ...ã€ã‚’è¡¨ç¤º
    const currentPending = pendingTextRef.current;
    setInterimTranscript(currentPending ? currentPending + ' ğŸ¤...' : 'ğŸ¤ èªè­˜ä¸­...');

    try {
      console.log('[Whisper] Sending to API...');
      const result = await transcribeAudio(blob, apiKeyRef.current);
      console.log('[Whisper] Result:', result);
      
      if (result.text && result.text.trim()) {
        const newText = result.text.trim();
        // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¬„ã«è¿½åŠ 
        pendingTextRef.current = pendingTextRef.current 
          ? pendingTextRef.current + ' ' + newText 
          : newText;
        setInterimTranscript(pendingTextRef.current);
        setProcessingStatus('èªè­˜æˆåŠŸ');
      } else {
        setProcessingStatus('éŸ³å£°ãªã—ï¼ˆç„¡éŸ³ï¼‰');
      }
    } catch (e) {
      console.error('[Whisper] Transcription error:', e);
      setProcessingStatus('ã‚¨ãƒ©ãƒ¼: ' + (e instanceof Error ? e.message : 'ä¸æ˜'));
      if (e instanceof Error && e.message.includes('401')) {
        setError('OpenAI APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™ã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚');
      } else if (e instanceof Error && e.message.includes('429')) {
        setError('APIåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ãã ã•ã„ã€‚');
      }
    } finally {
      isProcessingRef.current = false;
    }
  }, []);

  // ä¸€å®šæ™‚é–“ã”ã¨ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¬„ã‹ã‚‰ä¼šè©±æ¬„ã«ç§»å‹•
  const flushToTranscript = useCallback(() => {
    if (pendingTextRef.current) {
      console.log('[Whisper] Flushing to transcript:', pendingTextRef.current);
      setTranscript((prev) => prev ? prev + '\n' + pendingTextRef.current : pendingTextRef.current);
      pendingTextRef.current = '';
      setInterimTranscript('');
    }
  }, []);

  const startListening = useCallback(async () => {
    if (!isSupported) {
      setError('éŸ³å£°éŒ²éŸ³ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“');
      return;
    }

    // APIã‚­ãƒ¼ãƒã‚§ãƒƒã‚¯
    const key = apiKeyRef.current;
    if (!key || key.includes('XXXX') || key.length < 10) {
      setError('OpenAI APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„');
      return;
    }

    setError(null);
    setState('starting');
    pendingTextRef.current = '';
    setProcessingStatus('é–‹å§‹ä¸­...');

    try {
      const recorder = new AudioRecorder();
      recorder.setGain(currentGain);
      
      await recorder.start((level) => {
        setAudioLevel(level);
        // ã‚ˆã‚Šä½ã„é–¾å€¤ã§éŸ³å£°æ¤œå‡º
        setIsSpeechDetected(level > 0.02);
      });

      recorderRef.current = recorder;
      setState('listening');
      setProcessingStatus('éŒ²éŸ³ä¸­');

      // å®šæœŸçš„ã«éŸ³å£°ã‚’å‡¦ç†ï¼ˆ1.5ç§’ã”ã¨ï¼‰
      intervalRef.current = setInterval(() => {
        processAudio();
      }, intervalMs);

      // 8ç§’ã”ã¨ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¬„ã‹ã‚‰ä¼šè©±æ¬„ã«ç§»å‹•
      flushIntervalRef.current = setInterval(() => {
        flushToTranscript();
      }, 8000);

    } catch (e) {
      console.error('[Whisper] Failed to start:', e);
      setError('ãƒã‚¤ã‚¯ã®ä½¿ç”¨ãŒè¨±å¯ã•ã‚Œã¦ã„ã¾ã›ã‚“');
      setState('idle');
      setProcessingStatus('');
    }
  }, [isSupported, currentGain, intervalMs, processAudio, flushToTranscript]);

  const stopListening = useCallback(async () => {
    setState('stopping');
    setProcessingStatus('åœæ­¢ä¸­...');

    // ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«ã‚’åœæ­¢
    if (intervalRef.current) {
      clearInterval(intervalRef.current);
      intervalRef.current = null;
    }
    if (flushIntervalRef.current) {
      clearInterval(flushIntervalRef.current);
      flushIntervalRef.current = null;
    }

    // æœ€å¾Œã®éŸ³å£°ã‚’å‡¦ç†
    if (recorderRef.current) {
      const finalBlob = recorderRef.current.stop();
      
      if (finalBlob && finalBlob.size > 100) {
        setState('processing');
        setInterimTranscript('æœ€çµ‚å‡¦ç†ä¸­...');
        setProcessingStatus('æœ€çµ‚å‡¦ç†ä¸­...');
        
        try {
          const result = await transcribeAudio(finalBlob, apiKeyRef.current);
          if (result.text && result.text.trim()) {
            pendingTextRef.current = pendingTextRef.current 
              ? pendingTextRef.current + ' ' + result.text.trim() 
              : result.text.trim();
          }
        } catch (e) {
          console.error('[Whisper] Final transcription error:', e);
        }
      }
      
      recorderRef.current = null;
    }

    // æ®‹ã‚Šã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¼šè©±æ¬„ã«ç§»å‹•
    flushToTranscript();

    setState('idle');
    setInterimTranscript('');
    setIsSpeechDetected(false);
    setAudioLevel(0);
    setProcessingStatus('');
  }, [flushToTranscript]);

  const clearTranscript = useCallback(() => {
    setTranscript('');
    setInterimTranscript('');
    pendingTextRef.current = '';
  }, []);

  // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
  useEffect(() => {
    return () => {
      if (intervalRef.current) {
        clearInterval(intervalRef.current);
      }
      if (flushIntervalRef.current) {
        clearInterval(flushIntervalRef.current);
      }
      if (recorderRef.current) {
        recorderRef.current.stop();
      }
    };
  }, []);

  return {
    transcript,
    interimTranscript,
    state,
    isListening: state === 'listening' || state === 'processing',
    isSpeechDetected,
    error,
    isSupported,
    audioLevel,
    currentGain,
    processingStatus,
    setGain,
    startListening,
    stopListening,
    clearTranscript,
  };
}

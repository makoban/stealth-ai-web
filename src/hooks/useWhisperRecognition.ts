import { useState, useEffect, useRef, useCallback } from 'react';
import { AudioRecorder, transcribeAudio, OPENAI_API_KEY } from '../lib/whisper';
import { correctRealtimeText, ConversationGenre, HARDCODED_API_KEY } from '../lib/gemini';

export type RecognitionState = 'idle' | 'starting' | 'listening' | 'processing' | 'stopping';

export interface UseWhisperRecognitionOptions {
  apiKey?: string;
  intervalMs?: number; // éŸ³å£°ã‚’é€ä¿¡ã™ã‚‹é–“éš”ï¼ˆãƒŸãƒªç§’ï¼‰
  silenceThreshold?: number; // ç„¡éŸ³ã¨åˆ¤å®šã™ã‚‹é–¾å€¤ï¼ˆ0-1ï¼‰
}

// Whisperã®å¹»è¦šï¼ˆhallucinationï¼‰ã¨ã—ã¦ã‚ˆãå‡ºã‚‹ãƒ•ãƒ¬ãƒ¼ã‚º
// å®Œå…¨ä¸€è‡´ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ã‚º
const HALLUCINATION_EXACT = [
  'ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ',
  'ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™',
  'ã”è¦§ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ',
  'ã”è¦§ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™',
  'æœ¬æ—¥ã¯ã”è¦§ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™',
  'æœ¬æ—¥ã¯ã”è¦§ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ',
  'ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ',
  'ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™',
  'ãŠç–²ã‚Œæ§˜ã§ã—ãŸ',
  'ã‚ˆã„ä¸€æ—¥ã‚’',
  'è‰¯ã„ä¸€æ—¥ã‚’',
  'ãŠã‚„ã™ã¿ãªã•ã„',
  'ã•ã‚ˆã†ãªã‚‰',
  'ã¾ãŸã­',
  'ãƒã‚¤ãƒã‚¤',
  'çµ‚ã‚ã‚Š',
  'ãŠã—ã¾ã„',
  'Thank you for watching',
  'Thanks for watching',
  'Subscribe',
  'Like and subscribe',
  'MochiMochi',
  'Amara.org',
  'www.',
  'http',
  '.com',
  '.jp',
  '...',
  'ã€‚ã€‚ã€‚',
  'â€¦',
];

// éƒ¨åˆ†ä¸€è‡´ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ã‚º
const HALLUCINATION_PARTIAL = [
  'ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²',
  'é«˜è©•ä¾¡ã¨ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²',
  'å­—å¹•',
  'subtitles',
  'ã”è¦–è´',
  'è¦–è´',
  'ã”è¦§ã„ãŸã ã',
  'ã”è¦§é ‚ã',
  'ãŠè´ã',
  'ãŠèã',
  'æ¬¡å›',
  'æ¬¡ã®å‹•ç”»',
  'ã¾ãŸä¼šã„ã¾ã—ã‚‡ã†',
  'ãŠæ¥½ã—ã¿ã«',
  'æä¾›',
  'ã‚¹ãƒãƒ³ã‚µãƒ¼',
  'åºƒå‘Š',
  'CM',
  'ã‚³ãƒãƒ¼ã‚·ãƒ£ãƒ«',
];

// å¹»è¦šãƒ•ãƒ¬ãƒ¼ã‚ºã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
function isHallucination(text: string): boolean {
  const normalized = text.trim();
  const normalizedLower = normalized.toLowerCase();
  
  // å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
  for (const phrase of HALLUCINATION_EXACT) {
    if (normalized === phrase || normalizedLower === phrase.toLowerCase()) {
      console.log('[Whisper] Hallucination detected (exact):', normalized);
      return true;
    }
  }
  
  // éƒ¨åˆ†ä¸€è‡´ãƒã‚§ãƒƒã‚¯
  for (const phrase of HALLUCINATION_PARTIAL) {
    if (normalizedLower.includes(phrase.toLowerCase())) {
      console.log('[Whisper] Hallucination detected (partial):', normalized, 'matched:', phrase);
      return true;
    }
  }
  
  // çŸ­ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯ãƒã‚¤ã‚ºã®å¯èƒ½æ€§ãŒé«˜ã„ï¼ˆ4æ–‡å­—ä»¥ä¸‹ï¼‰
  if (normalized.length <= 4) {
    console.log('[Whisper] Hallucination detected (too short):', normalized);
    return true;
  }
  
  // ã€Œï¼ã€ã§çµ‚ã‚ã‚‹çŸ­ã„ãƒ•ãƒ¬ãƒ¼ã‚ºã¯å¹»è¦šã®å¯èƒ½æ€§ãŒé«˜ã„
  if (normalized.endsWith('!') && normalized.length < 15) {
    console.log('[Whisper] Hallucination detected (short exclamation):', normalized);
    return true;
  }
  
  // åŒã˜æ–‡å­—ã®ç¹°ã‚Šè¿”ã—ï¼ˆä¾‹: "ã‚ã‚ã‚ã‚", "ã‚“ã‚“ã‚“ã‚“"ï¼‰
  if (/^(.)\1{3,}$/.test(normalized)) {
    console.log('[Whisper] Hallucination detected (repeated char):', normalized);
    return true;
  }
  
  // éŸ³æ¥½è¨˜å·ã‚„ç‰¹æ®Šæ–‡å­—ã®ã¿
  if (/^[â™ªâ™«â™¬â™­â™®â™¯â™©â—â—‹â– â–¡â–²â–³â˜…â˜†â€»â†’â†â†‘â†“ã€€ ]+$/.test(normalized)) {
    console.log('[Whisper] Hallucination detected (special chars only):', normalized);
    return true;
  }
  
  return false;
}

// ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ•´å½¢çµæœã®å‹
export interface RealtimeTextResult {
  correctedText: string;
  originalText: string;
  wasModified: boolean;
  detectedProperNouns: string[];
}

export function useWhisperRecognition(options: UseWhisperRecognitionOptions = {}) {
  const {
    apiKey = OPENAI_API_KEY,
    intervalMs = 4000, // 4ç§’ã”ã¨ã«é€ä¿¡
    silenceThreshold = 0.05, // 5%ä»¥ä¸‹ã¯ç„¡éŸ³ã¨åˆ¤å®š
  } = options;

  const [transcript, setTranscript] = useState<string>('');
  const [interimTranscript, setInterimTranscript] = useState<string>('');
  const [state, setState] = useState<RecognitionState>('idle');
  const [error, setError] = useState<string | null>(null);
  const [isSupported, setIsSupported] = useState<boolean>(false);
  const [isSpeechDetected, setIsSpeechDetected] = useState<boolean>(false);
  const [audioLevel, setAudioLevel] = useState<number>(0);
  const [isClipping, setIsClipping] = useState<boolean>(false);
  const [currentGain, setCurrentGain] = useState<number>(50); // åˆæœŸå€¤ã¯æœ€å¤§
  const [processingStatus, setProcessingStatus] = useState<string>('');
  
  // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ•´å½¢ç”¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
  const [conversationContext, setConversationContext] = useState<string>('');
  const [currentGenre, setCurrentGenre] = useState<ConversationGenre | null>(null);
  
  // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ•´å½¢çµæœã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
  const [onRealtimeCorrection, setOnRealtimeCorrection] = useState<((result: RealtimeTextResult) => void) | null>(null);

  const recorderRef = useRef<AudioRecorder | null>(null);
  const intervalRef = useRef<ReturnType<typeof setInterval> | null>(null);
  const flushIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);
  const isProcessingRef = useRef<boolean>(false);
  const pendingTextRef = useRef<string>('');
  const pendingOriginalTextRef = useRef<string>(''); // æ•´å½¢å‰ã®ãƒ†ã‚­ã‚¹ãƒˆ
  const apiKeyRef = useRef<string>(apiKey);
  const recentAudioLevelsRef = useRef<number[]>([]); // æœ€è¿‘ã®éŸ³å£°ãƒ¬ãƒ™ãƒ«ã‚’è¨˜éŒ²
  const maxAudioLevelRef = useRef<number>(0); // æœŸé–“ä¸­ã®æœ€å¤§éŸ³å£°ãƒ¬ãƒ™ãƒ«
  const conversationContextRef = useRef<string>('');
  const currentGenreRef = useRef<ConversationGenre | null>(null);

  // APIã‚­ãƒ¼ã‚’refã§ä¿æŒï¼ˆå†ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚’é˜²ãï¼‰
  useEffect(() => {
    apiKeyRef.current = apiKey;
  }, [apiKey]);

  // ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’refã§ä¿æŒ
  useEffect(() => {
    conversationContextRef.current = conversationContext;
  }, [conversationContext]);

  // ã‚¸ãƒ£ãƒ³ãƒ«ã‚’refã§ä¿æŒ
  useEffect(() => {
    currentGenreRef.current = currentGenre;
  }, [currentGenre]);

  // ã‚µãƒãƒ¼ãƒˆç¢ºèª
  useEffect(() => {
    const supported = typeof navigator.mediaDevices !== 'undefined' && 
      typeof navigator.mediaDevices.getUserMedia === 'function';
    setIsSupported(supported);
    if (!supported) {
      setError('ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯éŸ³å£°éŒ²éŸ³ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚');
    }
  }, []);

  // ã‚²ã‚¤ãƒ³å€¤ã®å¤‰æ›´ï¼ˆéŒ²éŸ³ä¸­ã§ã‚‚ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã«åæ˜ ï¼‰
  const setGain = useCallback((value: number) => {
    setCurrentGain(value);
    if (recorderRef.current) {
      recorderRef.current.setGain(value);
    }
  }, []);

  // ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ›´æ–°é–¢æ•°
  const updateContext = useCallback((context: string) => {
    setConversationContext(context);
    conversationContextRef.current = context;
  }, []);

  // ã‚¸ãƒ£ãƒ³ãƒ«æ›´æ–°é–¢æ•°
  const updateGenre = useCallback((genre: ConversationGenre | null) => {
    setCurrentGenre(genre);
    currentGenreRef.current = genre;
  }, []);

  // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ•´å½¢ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
  const setRealtimeCorrectionCallback = useCallback((callback: ((result: RealtimeTextResult) => void) | null) => {
    setOnRealtimeCorrection(() => callback);
  }, []);

  // å®šæœŸçš„ã«éŸ³å£°ã‚’é€ä¿¡ã—ã¦æ–‡å­—èµ·ã“ã—
  const processAudio = useCallback(async () => {
    if (!recorderRef.current) {
      console.log('[Whisper] No recorder');
      return;
    }
    if (isProcessingRef.current) {
      console.log('[Whisper] Already processing');
      return;
    }
    if (!recorderRef.current.isRecording()) {
      console.log('[Whisper] Not recording');
      return;
    }

    // æœ€å¤§éŸ³å£°ãƒ¬ãƒ™ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆç„¡éŸ³ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
    const maxLevel = maxAudioLevelRef.current;
    console.log('[Whisper] Max audio level in period:', maxLevel);
    
    if (maxLevel < silenceThreshold) {
      console.log('[Whisper] Silence detected, skipping API call');
      setProcessingStatus(`ç„¡éŸ³æ¤œå‡ºï¼ˆãƒ¬ãƒ™ãƒ«: ${(maxLevel * 100).toFixed(0)}%ï¼‰`);
      // ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ã—ã¦æ¬¡ã®æœŸé–“ã¸
      recorderRef.current.getIntermediateBlob();
      maxAudioLevelRef.current = 0;
      recentAudioLevelsRef.current = [];
      return;
    }

    const blob = recorderRef.current.getIntermediateBlob();
    console.log('[Whisper] Got blob:', blob?.size || 0, 'bytes');
    
    // æœ€å¤§ãƒ¬ãƒ™ãƒ«ã‚’ãƒªã‚»ãƒƒãƒˆ
    maxAudioLevelRef.current = 0;
    recentAudioLevelsRef.current = [];
    
    // æœ€å°ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆWAVãƒ˜ãƒƒãƒ€ãƒ¼44ãƒã‚¤ãƒˆ + æœ€ä½é™ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
    if (!blob || blob.size < 1000) {
      setProcessingStatus('éŸ³å£°ãƒ‡ãƒ¼ã‚¿ä¸è¶³');
      return;
    }

    isProcessingRef.current = true;
    setProcessingStatus('Whisper APIã«é€ä¿¡ä¸­...');
    
    // å‡¦ç†ä¸­ã¯ã€Œ...ã€ã‚’è¡¨ç¤º
    const currentPending = pendingTextRef.current;
    setInterimTranscript(currentPending ? currentPending + ' ğŸ¤...' : 'ğŸ¤ èªè­˜ä¸­...');

    try {
      console.log('[Whisper] Sending to API...');
      const result = await transcribeAudio(blob, apiKeyRef.current);
      console.log('[Whisper] Result:', result);
      
      if (result.text && result.text.trim()) {
        const rawText = result.text.trim();
        
        // å¹»è¦šãƒ•ãƒ¬ãƒ¼ã‚ºã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if (isHallucination(rawText)) {
          console.log('[Whisper] Filtered hallucination:', rawText);
          setProcessingStatus('ãƒã‚¤ã‚ºé™¤å»ï¼ˆå¹»è¦šãƒ•ã‚£ãƒ«ã‚¿ï¼‰');
        } else {
          // Geminiã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ•´å½¢
          setProcessingStatus('Geminiã§æ•´å½¢ä¸­...');
          console.log('[Whisper] Correcting with Gemini...');
          
          try {
            const correctionResult = await correctRealtimeText(
              rawText,
              conversationContextRef.current,
              currentGenreRef.current,
              HARDCODED_API_KEY
            );
            
            console.log('[Whisper] Correction result:', correctionResult);
            
            const correctedText = correctionResult.correctedText;
            
            // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¬„ã«æ•´å½¢å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
            pendingTextRef.current = pendingTextRef.current 
              ? pendingTextRef.current + ' ' + correctedText 
              : correctedText;
            
            // æ•´å½¢å‰ã®ãƒ†ã‚­ã‚¹ãƒˆã‚‚ä¿æŒ
            pendingOriginalTextRef.current = pendingOriginalTextRef.current
              ? pendingOriginalTextRef.current + ' ' + rawText
              : rawText;
            
            setInterimTranscript(pendingTextRef.current);
            
            if (correctionResult.wasModified) {
              setProcessingStatus(`æ•´å½¢å®Œäº†: ${rawText} â†’ ${correctedText}`);
            } else {
              setProcessingStatus('èªè­˜æˆåŠŸ: ' + correctedText.substring(0, 20) + '...');
            }
            
            // ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§å›ºæœ‰åè©æƒ…å ±ã‚’é€šçŸ¥
            if (onRealtimeCorrection) {
              onRealtimeCorrection({
                correctedText: correctedText,
                originalText: rawText,
                wasModified: correctionResult.wasModified,
                detectedProperNouns: correctionResult.detectedProperNouns,
              });
            }
          } catch (geminiError) {
            console.error('[Whisper] Gemini correction error:', geminiError);
            // Geminiã‚¨ãƒ©ãƒ¼æ™‚ã¯ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨
            pendingTextRef.current = pendingTextRef.current 
              ? pendingTextRef.current + ' ' + rawText 
              : rawText;
            pendingOriginalTextRef.current = pendingOriginalTextRef.current
              ? pendingOriginalTextRef.current + ' ' + rawText
              : rawText;
            setInterimTranscript(pendingTextRef.current);
            setProcessingStatus('èªè­˜æˆåŠŸï¼ˆæ•´å½¢ã‚¹ã‚­ãƒƒãƒ—ï¼‰: ' + rawText.substring(0, 20) + '...');
          }
        }
      } else {
        setProcessingStatus('éŸ³å£°ãªã—ï¼ˆç„¡éŸ³ï¼‰');
      }
    } catch (e) {
      console.error('[Whisper] Transcription error:', e);
      setProcessingStatus('ã‚¨ãƒ©ãƒ¼: ' + (e instanceof Error ? e.message : 'ä¸æ˜'));
      if (e instanceof Error && e.message.includes('401')) {
        setError('OpenAI APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™ã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚');
      } else if (e instanceof Error && e.message.includes('429')) {
        setError('APIåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ãã ã•ã„ã€‚');
      }
    } finally {
      isProcessingRef.current = false;
    }
  }, [silenceThreshold, onRealtimeCorrection]);

  // ä¸€å®šæ™‚é–“ã”ã¨ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¬„ã‹ã‚‰ä¼šè©±æ¬„ã«ç§»å‹•
  const flushToTranscript = useCallback(() => {
    if (pendingTextRef.current && pendingTextRef.current.trim()) {
      const textToFlush = pendingTextRef.current.trim();
      const originalToFlush = pendingOriginalTextRef.current.trim();
      console.log('[Whisper] Flushing to transcript:', textToFlush);
      console.log('[Whisper] Original text:', originalToFlush);
      
      // ä¼šè©±æ¬„ã«è¿½åŠ ï¼ˆæ•´å½¢å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆï¼‰
      setTranscript((prev) => {
        const newTranscript = prev ? prev + '\n' + textToFlush : textToFlush;
        console.log('[Whisper] New transcript:', newTranscript);
        return newTranscript;
      });
      
      // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¬„ã‚’ã‚¯ãƒªã‚¢
      pendingTextRef.current = '';
      pendingOriginalTextRef.current = '';
      setInterimTranscript('');
    }
  }, []);

  const startListening = useCallback(async () => {
    if (!isSupported) {
      setError('éŸ³å£°éŒ²éŸ³ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“');
      return;
    }

    // APIã‚­ãƒ¼ãƒã‚§ãƒƒã‚¯
    const key = apiKeyRef.current;
    if (!key || key.includes('XXXX') || key.length < 10) {
      setError('OpenAI APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„');
      return;
    }

    setError(null);
    setState('starting');
    pendingTextRef.current = '';
    pendingOriginalTextRef.current = '';
    maxAudioLevelRef.current = 0;
    recentAudioLevelsRef.current = [];
    setProcessingStatus('é–‹å§‹ä¸­...');

    try {
      const recorder = new AudioRecorder();
      recorder.setGain(currentGain);
      
      await recorder.start((level, clipping) => {
        setAudioLevel(level);
        setIsClipping(clipping);
        // æœ€å¤§ãƒ¬ãƒ™ãƒ«ã‚’æ›´æ–°
        if (level > maxAudioLevelRef.current) {
          maxAudioLevelRef.current = level;
        }
        recentAudioLevelsRef.current.push(level);
        // æœ€æ–°100ä»¶ã®ã¿ä¿æŒ
        if (recentAudioLevelsRef.current.length > 100) {
          recentAudioLevelsRef.current.shift();
        }
        // ã‚ˆã‚Šä½ã„é–¾å€¤ã§éŸ³å£°æ¤œå‡º
        setIsSpeechDetected(level > 0.02);
      });

      recorderRef.current = recorder;
      setState('listening');
      setProcessingStatus('è§£æä¸­');

      // å®šæœŸçš„ã«éŸ³å£°ã‚’å‡¦ç†
      intervalRef.current = setInterval(() => {
        processAudio();
      }, intervalMs);

      // 6ç§’ã”ã¨ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¬„ã‹ã‚‰ä¼šè©±æ¬„ã«ç§»å‹•
      flushIntervalRef.current = setInterval(() => {
        flushToTranscript();
      }, 6000);

    } catch (e) {
      console.error('[Whisper] Failed to start:', e);
      setError('ãƒã‚¤ã‚¯ã®ä½¿ç”¨ãŒè¨±å¯ã•ã‚Œã¦ã„ã¾ã›ã‚“');
      setState('idle');
      setProcessingStatus('');
    }
  }, [isSupported, currentGain, intervalMs, processAudio, flushToTranscript]);

  const stopListening = useCallback(async () => {
    setState('stopping');
    setProcessingStatus('åœæ­¢ä¸­...');

    // ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«ã‚’åœæ­¢
    if (intervalRef.current) {
      clearInterval(intervalRef.current);
      intervalRef.current = null;
    }
    if (flushIntervalRef.current) {
      clearInterval(flushIntervalRef.current);
      flushIntervalRef.current = null;
    }

    // æœ€å¾Œã®éŸ³å£°ã‚’å‡¦ç†
    if (recorderRef.current) {
      const finalBlob = recorderRef.current.stop();
      
      // ç„¡éŸ³ã§ãªãã€ååˆ†ãªã‚µã‚¤ã‚ºãŒã‚ã‚‹å ´åˆã®ã¿å‡¦ç†
      if (finalBlob && finalBlob.size > 1000 && maxAudioLevelRef.current >= silenceThreshold) {
        setState('processing');
        setInterimTranscript('æœ€çµ‚å‡¦ç†ä¸­...');
        setProcessingStatus('æœ€çµ‚å‡¦ç†ä¸­...');
        
        try {
          const result = await transcribeAudio(finalBlob, apiKeyRef.current);
          if (result.text && result.text.trim() && !isHallucination(result.text.trim())) {
            const rawText = result.text.trim();
            
            // æœ€çµ‚ãƒ†ã‚­ã‚¹ãƒˆã‚‚Geminiã§æ•´å½¢
            try {
              const correctionResult = await correctRealtimeText(
                rawText,
                conversationContextRef.current,
                currentGenreRef.current,
                HARDCODED_API_KEY
              );
              
              pendingTextRef.current = pendingTextRef.current 
                ? pendingTextRef.current + ' ' + correctionResult.correctedText 
                : correctionResult.correctedText;
              pendingOriginalTextRef.current = pendingOriginalTextRef.current
                ? pendingOriginalTextRef.current + ' ' + rawText
                : rawText;
            } catch {
              pendingTextRef.current = pendingTextRef.current 
                ? pendingTextRef.current + ' ' + rawText 
                : rawText;
              pendingOriginalTextRef.current = pendingOriginalTextRef.current
                ? pendingOriginalTextRef.current + ' ' + rawText
                : rawText;
            }
          }
        } catch (e) {
          console.error('[Whisper] Final transcription error:', e);
        }
      }
      
      recorderRef.current = null;
    }

    // æ®‹ã‚Šã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¼šè©±æ¬„ã«ç§»å‹•
    flushToTranscript();

    setState('idle');
    setInterimTranscript('');
    setIsSpeechDetected(false);
    setAudioLevel(0);
    setProcessingStatus('');
    maxAudioLevelRef.current = 0;
    recentAudioLevelsRef.current = [];
  }, [flushToTranscript, silenceThreshold]);

  const clearTranscript = useCallback(() => {
    setTranscript('');
    setInterimTranscript('');
    pendingTextRef.current = '';
    pendingOriginalTextRef.current = '';
    setConversationContext('');
    conversationContextRef.current = '';
  }, []);

  // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
  useEffect(() => {
    return () => {
      if (intervalRef.current) {
        clearInterval(intervalRef.current);
      }
      if (flushIntervalRef.current) {
        clearInterval(flushIntervalRef.current);
      }
      if (recorderRef.current) {
        recorderRef.current.stop();
      }
    };
  }, []);

  return {
    transcript,
    interimTranscript,
    state,
    isListening: state === 'listening' || state === 'processing',
    isSpeechDetected,
    isClipping,
    error,
    isSupported,
    audioLevel,
    currentGain,
    processingStatus,
    setGain,
    startListening,
    stopListening,
    clearTranscript,
    // æ–°ã—ã„æ©Ÿèƒ½
    updateContext,
    updateGenre,
    setRealtimeCorrectionCallback,
  };
}

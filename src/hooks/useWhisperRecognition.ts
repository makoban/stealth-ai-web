import { useState, useEffect, useRef, useCallback } from 'react';
import { AudioRecorder, transcribeAudio } from '../lib/whisper';

export type RecognitionState = 'idle' | 'starting' | 'listening' | 'processing' | 'stopping';

export interface UseWhisperRecognitionOptions {
  intervalMs?: number; // éŸ³å£°ã‚’é€ä¿¡ã™ã‚‹é–“éš”ï¼ˆãƒŸãƒªç§’ï¼‰
  silenceThreshold?: number; // ç„¡éŸ³ã¨åˆ¤å®šã™ã‚‹é–¾å€¤ï¼ˆ0-1ï¼‰
  whisperPrompt?: string; // Whisper APIã«æ¸¡ã™ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå›ºæœ‰åè©ã®ãƒ’ãƒ³ãƒˆï¼‰
  onBufferReady?: (text: string) => void; // ãƒãƒƒãƒ•ã‚¡ãŒæº–å‚™ã§ããŸæ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆGeminié€ä¿¡ç”¨ï¼‰
}

// Whisperã®å¹»è¦šï¼ˆhallucinationï¼‰ã¨ã—ã¦ã‚ˆãå‡ºã‚‹ãƒ•ãƒ¬ãƒ¼ã‚º
// å®Œå…¨ä¸€è‡´ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ã‚º
const HALLUCINATION_EXACT = [
  'ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ',
  'ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™',
  'ã”è¦§ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ',
  'ã”è¦§ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™',
  'æœ¬æ—¥ã¯ã”è¦§ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™',
  'æœ¬æ—¥ã¯ã”è¦§ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ',
  'ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ',
  'ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™',
  'ãŠç–²ã‚Œæ§˜ã§ã—ãŸ',
  'ã‚ˆã„ä¸€æ—¥ã‚’',
  'è‰¯ã„ä¸€æ—¥ã‚’',
  'ãŠã‚„ã™ã¿ãªã•ã„',
  'ã•ã‚ˆã†ãªã‚‰',
  'ã¾ãŸã­',
  'ãƒã‚¤ãƒã‚¤',
  'çµ‚ã‚ã‚Š',
  'ãŠã—ã¾ã„',
  'Thank you for watching',
  'Thanks for watching',
  'Subscribe',
  'Like and subscribe',
  'MochiMochi',
  'Amara.org',
  'www.',
  'http',
  '.com',
  '.jp',
  '...',
  'ã€‚ã€‚ã€‚',
  'â€¦',
];

// éƒ¨åˆ†ä¸€è‡´ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ã‚º
const HALLUCINATION_PARTIAL = [
  'ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²',
  'é«˜è©•ä¾¡ã¨ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²',
  'å­—å¹•',
  'subtitles',
  'ã”è¦–è´',
  'è¦–è´',
  'ã”è¦§ã„ãŸã ã',
  'ã”è¦§é ‚ã',
  'ãŠè´ã',
  'ãŠèã',
  'æ¬¡å›',
  'æ¬¡ã®å‹•ç”»',
  'ã¾ãŸä¼šã„ã¾ã—ã‚‡ã†',
  'ãŠæ¥½ã—ã¿ã«',
  'æä¾›',
  'ã‚¹ãƒãƒ³ã‚µãƒ¼',
  'åºƒå‘Š',
  'CM',
  'ã‚³ãƒãƒ¼ã‚·ãƒ£ãƒ«',
];

// å¹»è¦šãƒ•ãƒ¬ãƒ¼ã‚ºã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
function isHallucination(text: string): boolean {
  const normalized = text.trim();
  const normalizedLower = normalized.toLowerCase();
  
  // å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
  for (const phrase of HALLUCINATION_EXACT) {
    if (normalized === phrase || normalizedLower === phrase.toLowerCase()) {
      console.log('[Whisper] Hallucination detected (exact):', normalized);
      return true;
    }
  }
  
  // éƒ¨åˆ†ä¸€è‡´ãƒã‚§ãƒƒã‚¯
  for (const phrase of HALLUCINATION_PARTIAL) {
    if (normalizedLower.includes(phrase.toLowerCase())) {
      console.log('[Whisper] Hallucination detected (partial):', normalized, 'matched:', phrase);
      return true;
    }
  }
  
  // çŸ­ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯ãƒã‚¤ã‚ºã®å¯èƒ½æ€§ãŒé«˜ã„ï¼ˆ4æ–‡å­—ä»¥ä¸‹ï¼‰
  if (normalized.length <= 4) {
    console.log('[Whisper] Hallucination detected (too short):', normalized);
    return true;
  }
  
  // ã€Œï¼ã€ã§çµ‚ã‚ã‚‹çŸ­ã„ãƒ•ãƒ¬ãƒ¼ã‚ºã¯å¹»è¦šã®å¯èƒ½æ€§ãŒé«˜ã„
  if (normalized.endsWith('!') && normalized.length < 15) {
    console.log('[Whisper] Hallucination detected (short exclamation):', normalized);
    return true;
  }
  
  // åŒã˜æ–‡å­—ã®ç¹°ã‚Šè¿”ã—ï¼ˆä¾‹: "ã‚ã‚ã‚ã‚", "ã‚“ã‚“ã‚“ã‚“"ï¼‰
  if (/^(.)\1{3,}$/.test(normalized)) {
    console.log('[Whisper] Hallucination detected (repeated char):', normalized);
    return true;
  }
  
  // éŸ³æ¥½è¨˜å·ã‚„ç‰¹æ®Šæ–‡å­—ã®ã¿
  if (/^[â™ªâ™«â™¬â™­â™®â™¯â™©â—â—‹â– â–¡â–²â–³â˜…â˜†â€»â†’â†â†‘â†“ã€€ ]+$/.test(normalized)) {
    console.log('[Whisper] Hallucination detected (special chars only):', normalized);
    return true;
  }
  
  return false;
}

export function useWhisperRecognition(options: UseWhisperRecognitionOptions = {}) {
  const {
    silenceThreshold = 0.05, // 5%ä»¥ä¸‹ã¯ç„¡éŸ³ã¨åˆ¤å®š
    whisperPrompt = '', // Whisperã«æ¸¡ã™ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    onBufferReady, // ãƒãƒƒãƒ•ã‚¡æº–å‚™å®Œäº†ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
  } = options;

  const [transcript, setTranscript] = useState<string>('');
  const [interimTranscript, setInterimTranscript] = useState<string>('');
  const [state, setState] = useState<RecognitionState>('idle');
  const [error, setError] = useState<string | null>(null);
  const [isSupported, setIsSupported] = useState<boolean>(false);
  const [isSpeechDetected, setIsSpeechDetected] = useState<boolean>(false);
  const [audioLevel, setAudioLevel] = useState<number>(0);
  const [isClipping, setIsClipping] = useState<boolean>(false);
  const [currentGain, setCurrentGain] = useState<number>(50); // åˆæœŸå€¤ã¯æœ€å¤§
  const [processingStatus, setProcessingStatus] = useState<string>('');

  const recorderRef = useRef<AudioRecorder | null>(null);
  const isProcessingRef = useRef<boolean>(false);
  const pendingTextRef = useRef<string>('');
  
  // ã‚¿ã‚¤ãƒ—ãƒ©ã‚¤ã‚¿ãƒ¼ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ç”¨
  const animationTimerRef = useRef<ReturnType<typeof setTimeout> | null>(null);
  const displayedTextRef = useRef<string>(''); // ç¾åœ¨è¡¨ç¤ºä¸­ã®ãƒ†ã‚­ã‚¹ãƒˆ
  const targetTextRef = useRef<string>(''); // ç›®æ¨™ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚¿ã‚¤ãƒ—ãƒ©ã‚¤ã‚¿ãƒ¼ç”¨ï¼‰
  
  // ãƒ€ãƒ–ãƒ«ãƒãƒƒãƒ•ã‚¡æ–¹å¼ï¼ˆå–ã‚Šã“ã¼ã—é˜²æ­¢ï¼‰
  const bufferARef = useRef<string>(''); // ãƒãƒƒãƒ•ã‚¡A
  const bufferBRef = useRef<string>(''); // ãƒãƒƒãƒ•ã‚¡B
  const activeBufferRef = useRef<'A' | 'B'>('A'); // ç¾åœ¨æ›¸ãè¾¼ã¿ä¸­ã®ãƒãƒƒãƒ•ã‚¡
  const bufferSilenceTimerRef = useRef<ReturnType<typeof setTimeout> | null>(null); // 0.4ç§’ç„¡éŸ³ã‚¿ã‚¤ãƒãƒ¼
  const BUFFER_SILENCE_DURATION = 400; // ãƒãƒƒãƒ•ã‚¡é€ä¿¡ã¾ã§ã®ç„¡éŸ³æ™‚é–“ï¼ˆ0.4ç§’ï¼‰
  
  const whisperPromptRef = useRef<string>(whisperPrompt);
  const onBufferReadyRef = useRef(onBufferReady);
  const recentAudioLevelsRef = useRef<number[]>([]); // æœ€è¿‘ã®éŸ³å£°ãƒ¬ãƒ™ãƒ«ã‚’è¨˜éŒ²
  const maxAudioLevelRef = useRef<number>(0); // æœŸé–“ä¸­ã®æœ€å¤§éŸ³å£°ãƒ¬ãƒ™ãƒ«
  
  // VADï¼ˆç„¡éŸ³æ¤œå‡ºï¼‰ç”¨ - ç„¡éŸ³0.4ç§’ã§é€ä¿¡
  const speechStartTimeRef = useRef<number | null>(null); // ç™ºè©±é–‹å§‹æ™‚åˆ»
  const silenceStartTimeRef = useRef<number | null>(null); // ç„¡éŸ³é–‹å§‹æ™‚åˆ»
  const vadTimeoutRef = useRef<ReturnType<typeof setTimeout> | null>(null); // VADã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
  const VAD_SILENCE_DURATION = 400; // ç„¡éŸ³ã¨åˆ¤å®šã™ã‚‹æ™‚é–“ï¼ˆ0.4ç§’ï¼‰
  const VAD_MIN_SPEECH_DURATION = 300; // æœ€ä½ç™ºè©±æ™‚é–“ï¼ˆ0.3ç§’ï¼‰
  const VAD_MAX_SPEECH_DURATION = 15000; // æœ€å¤§ç™ºè©±æ™‚é–“ï¼ˆ15ç§’ï¼‰
  const VAD_SPEECH_THRESHOLD = 0.015; // ç™ºè©±ã¨åˆ¤å®šã™ã‚‹é–¾å€¤ï¼ˆ1.5%ï¼‰

  // ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’refã§ä¿æŒ
  useEffect(() => {
    onBufferReadyRef.current = onBufferReady;
  }, [onBufferReady]);

  // ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’refã§ä¿æŒï¼ˆå†ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚’é˜²ãï¼‰
  useEffect(() => {
    whisperPromptRef.current = whisperPrompt;
    console.log('[Whisper] Prompt updated:', whisperPrompt?.slice(0, 50) + '...');
  }, [whisperPrompt]);

  // ã‚µãƒãƒ¼ãƒˆç¢ºèª
  useEffect(() => {
    const supported = typeof navigator.mediaDevices !== 'undefined' && 
      typeof navigator.mediaDevices.getUserMedia === 'function';
    setIsSupported(supported);
    if (!supported) {
      setError('ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯éŸ³å£°éŒ²éŸ³ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚');
    }
  }, []);

  // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒƒãƒ•ã‚¡ã‚’å–å¾—
  const getActiveBuffer = useCallback(() => {
    return activeBufferRef.current === 'A' ? bufferARef : bufferBRef;
  }, []);

  // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒƒãƒ•ã‚¡ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
  const appendToActiveBuffer = useCallback((text: string) => {
    const buffer = getActiveBuffer();
    if (buffer.current) {
      buffer.current += ' ' + text;
    } else {
      buffer.current = text;
    }
    console.log(`[Buffer] Appended to buffer ${activeBufferRef.current}:`, buffer.current);
  }, [getActiveBuffer]);

  // ãƒãƒƒãƒ•ã‚¡ã‚’åˆ‡ã‚Šæ›¿ãˆã¦Geminiã«é€ä¿¡
  const swapAndFlushBuffer = useCallback(() => {
    const currentBuffer = activeBufferRef.current;
    const bufferToSend = currentBuffer === 'A' ? bufferARef : bufferBRef;
    const textToSend = bufferToSend.current.trim();
    
    if (textToSend && onBufferReadyRef.current) {
      console.log(`[Buffer] Swapping: ${currentBuffer} -> ${currentBuffer === 'A' ? 'B' : 'A'}`);
      console.log(`[Buffer] Sending buffer ${currentBuffer} to Gemini:`, textToSend);
      
      // ãƒãƒƒãƒ•ã‚¡ã‚’åˆ‡ã‚Šæ›¿ãˆï¼ˆæ¬¡ã®Whisperçµæœã¯åˆ¥ã®ãƒãƒƒãƒ•ã‚¡ã«æ›¸ãè¾¼ã¾ã‚Œã‚‹ï¼‰
      activeBufferRef.current = currentBuffer === 'A' ? 'B' : 'A';
      
      // é€ä¿¡ã™ã‚‹ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢
      bufferToSend.current = '';
      
      // Geminiã«é€ä¿¡
      onBufferReadyRef.current(textToSend);
      
      // è¡¨ç¤ºã‚’ãƒªã‚»ãƒƒãƒˆ
      displayedTextRef.current = '';
      targetTextRef.current = '';
      if (animationTimerRef.current) {
        clearTimeout(animationTimerRef.current);
        animationTimerRef.current = null;
      }
      
      // æ–°ã—ã„ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒƒãƒ•ã‚¡ã®å†…å®¹ã‚’è¡¨ç¤º
      const newActiveBuffer = getActiveBuffer();
      if (newActiveBuffer.current) {
        setInterimTranscript(`ğŸ’¬ ${newActiveBuffer.current}`);
      } else {
        setInterimTranscript('ğŸ¤ æ¬¡ã®éŸ³å£°ã‚’å¾…æ©Ÿä¸­...');
      }
    }
  }, [getActiveBuffer]);

  // ã‚¿ã‚¤ãƒ—ãƒ©ã‚¤ã‚¿ãƒ¼ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹
  const startTypingAnimation = useCallback((newText: string) => {
    // æ—¢å­˜ã®ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
    if (animationTimerRef.current) {
      clearTimeout(animationTimerRef.current);
    }
    
    // ç›®æ¨™ãƒ†ã‚­ã‚¹ãƒˆã‚’è¨­å®š
    targetTextRef.current = newText;
    
    const animate = () => {
      const target = targetTextRef.current;
      const current = displayedTextRef.current;
      
      if (current.length < target.length) {
        // 1æ–‡å­—è¿½åŠ 
        displayedTextRef.current = target.slice(0, current.length + 1);
        setInterimTranscript(`ğŸ’¬ ${displayedTextRef.current}`);
        
        // æ¬¡ã®æ–‡å­—ã‚’è¡¨ç¤ºï¼ˆ50msé–“éš”ï¼‰
        animationTimerRef.current = setTimeout(animate, 50);
      }
    };
    
    animate();
  }, []);

  // ãƒãƒƒãƒ•ã‚¡ç„¡éŸ³ã‚¿ã‚¤ãƒãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ
  const resetBufferSilenceTimer = useCallback(() => {
    if (bufferSilenceTimerRef.current) {
      clearTimeout(bufferSilenceTimerRef.current);
    }
    
    // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒƒãƒ•ã‚¡ã«ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã®ã¿ã‚¿ã‚¤ãƒãƒ¼ã‚’è¨­å®š
    const activeBuffer = getActiveBuffer();
    if (activeBuffer.current.trim()) {
      bufferSilenceTimerRef.current = setTimeout(() => {
        swapAndFlushBuffer();
      }, BUFFER_SILENCE_DURATION);
    }
  }, [getActiveBuffer, swapAndFlushBuffer]);

  // ã‚²ã‚¤ãƒ³å€¤ã®å¤‰æ›´ï¼ˆéŒ²éŸ³ä¸­ã§ã‚‚ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã«åæ˜ ï¼‰
  const setGain = useCallback((value: number) => {
    setCurrentGain(value);
    if (recorderRef.current) {
      recorderRef.current.setGain(value);
    }
  }, []);

  // å®šæœŸçš„ã«éŸ³å£°ã‚’é€ä¿¡ã—ã¦æ–‡å­—èµ·ã“ã—
  const processAudio = useCallback(async () => {
    if (!recorderRef.current) {
      console.log('[Whisper] No recorder');
      return;
    }
    if (isProcessingRef.current) {
      console.log('[Whisper] Already processing');
      return;
    }
    if (!recorderRef.current.isRecording()) {
      console.log('[Whisper] Not recording');
      return;
    }

    // æœ€å¤§éŸ³å£°ãƒ¬ãƒ™ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆç„¡éŸ³ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
    const maxLevel = maxAudioLevelRef.current;
    console.log('[Whisper] Max audio level in period:', maxLevel);
    
    if (maxLevel < silenceThreshold) {
      console.log('[Whisper] Silence detected, skipping API call');
      setProcessingStatus(`ç„¡éŸ³æ¤œå‡ºï¼ˆãƒ¬ãƒ™ãƒ«: ${(maxLevel * 100).toFixed(0)}%ï¼‰`);
      // ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ã—ã¦æ¬¡ã®æœŸé–“ã¸
      recorderRef.current.getIntermediateBlob();
      maxAudioLevelRef.current = 0;
      recentAudioLevelsRef.current = [];
      return;
    }

    const blob = recorderRef.current.getIntermediateBlob();
    console.log('[Whisper] Got blob:', blob?.size || 0, 'bytes');
    
    // æœ€å¤§ãƒ¬ãƒ™ãƒ«ã‚’ãƒªã‚»ãƒƒãƒˆ
    maxAudioLevelRef.current = 0;
    recentAudioLevelsRef.current = [];
    
    // æœ€å°ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆWAVãƒ˜ãƒƒãƒ€ãƒ¼44ãƒã‚¤ãƒˆ + æœ€ä½é™ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
    if (!blob || blob.size < 1000) {
      setProcessingStatus('éŸ³å£°ãƒ‡ãƒ¼ã‚¿ä¸è¶³');
      return;
    }

    isProcessingRef.current = true;
    setProcessingStatus('Whisper APIã«é€ä¿¡ä¸­...');
    
    // å‡¦ç†ä¸­ã®è¡¨ç¤º
    const activeBuffer = getActiveBuffer();
    if (activeBuffer.current) {
      setInterimTranscript(`â˜ï¸ ${activeBuffer.current}...`);
    } else {
      setInterimTranscript('â˜ï¸ ã‚¯ãƒ©ã‚¦ãƒ‰ã§è§£æä¸­...');
    }

    try {
      console.log('[Whisper] Sending to API with prompt...');
      const result = await transcribeAudio(blob, whisperPromptRef.current);
      console.log('[Whisper] Result:', result);
      
      if (result.text && result.text.trim()) {
        const newText = result.text.trim();
        
        // å¹»è¦šãƒ•ãƒ¬ãƒ¼ã‚ºã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if (isHallucination(newText)) {
          console.log('[Whisper] Filtered hallucination:', newText);
          setProcessingStatus('ãƒã‚¤ã‚ºé™¤å»ï¼ˆå¹»è¦šãƒ•ã‚£ãƒ«ã‚¿ï¼‰');
          setInterimTranscript('ğŸ¤ æ¬¡ã®éŸ³å£°ã‚’å¾…æ©Ÿä¸­...');
        } else {
          // èªè­˜æˆåŠŸ
          console.log('[Whisper] Recognized text:', newText);
          
          // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
          appendToActiveBuffer(newText);
          
          // ã‚¿ã‚¤ãƒ—ãƒ©ã‚¤ã‚¿ãƒ¼ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹
          const currentActiveBuffer = getActiveBuffer();
          startTypingAnimation(currentActiveBuffer.current);
          
          // ãƒãƒƒãƒ•ã‚¡ç„¡éŸ³ã‚¿ã‚¤ãƒãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆ0.4ç§’å¾Œã«Geminié€ä¿¡ï¼‰
          resetBufferSilenceTimer();
          
          // ä¼šè©±æ¬„ã«ã‚‚è¿½åŠ ï¼ˆç”Ÿã®Whisperå‡ºåŠ›ï¼‰
          setTranscript((prev) => {
            const newTranscript = prev ? prev + '\n' + newText : newText;
            console.log('[Whisper] New transcript:', newTranscript);
            return newTranscript;
          });
          
          setProcessingStatus('èªè­˜æˆåŠŸ: ' + newText.substring(0, 20) + '...');
        }
      } else {
        setProcessingStatus('éŸ³å£°ãªã—ï¼ˆç„¡éŸ³ï¼‰');
        // ãƒãƒƒãƒ•ã‚¡ãŒã‚ã‚Œã°è¡¨ç¤ºã‚’ç¶­æŒ
        const activeBuffer = getActiveBuffer();
        if (!activeBuffer.current) {
          setInterimTranscript('ğŸ¤ æ¬¡ã®éŸ³å£°ã‚’å¾…æ©Ÿä¸­...');
        }
      }
    } catch (e) {
      console.error('[Whisper] Transcription error:', e);
      setProcessingStatus('ã‚¨ãƒ©ãƒ¼: ' + (e instanceof Error ? e.message : 'ä¸æ˜'));
      if (e instanceof Error && e.message.includes('401')) {
        setError('OpenAI APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™ã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚');
      } else if (e instanceof Error && e.message.includes('429')) {
        setError('APIåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ãã ã•ã„ã€‚');
      }
    } finally {
      isProcessingRef.current = false;
    }
  }, [silenceThreshold, getActiveBuffer, appendToActiveBuffer, startTypingAnimation, resetBufferSilenceTimer]);

  const startListening = useCallback(async () => {
    if (!isSupported) {
      setError('éŸ³å£°éŒ²éŸ³ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“');
      return;
    }

    setError(null);
    setState('starting');
    pendingTextRef.current = '';
    bufferARef.current = '';
    bufferBRef.current = '';
    activeBufferRef.current = 'A';
    displayedTextRef.current = '';
    targetTextRef.current = '';
    maxAudioLevelRef.current = 0;
    recentAudioLevelsRef.current = [];
    setProcessingStatus('é–‹å§‹ä¸­...');

    try {
      const recorder = new AudioRecorder();
      recorder.setGain(currentGain);
      
      await recorder.start((level, clipping) => {
        setAudioLevel(level);
        setIsClipping(clipping);
        // æœ€å¤§ãƒ¬ãƒ™ãƒ«ã‚’æ›´æ–°
        if (level > maxAudioLevelRef.current) {
          maxAudioLevelRef.current = level;
        }
        recentAudioLevelsRef.current.push(level);
        // æœ€æ–°100ä»¶ã®ã¿ä¿æŒ
        if (recentAudioLevelsRef.current.length > 100) {
          recentAudioLevelsRef.current.shift();
        }
        // ã‚ˆã‚Šä½ã„é–¾å€¤ã§éŸ³å£°æ¤œå‡º
        const isSpeaking = level > VAD_SPEECH_THRESHOLD;
        setIsSpeechDetected(isSpeaking);
        
        // VADãƒ­ã‚¸ãƒƒã‚¯
        const now = Date.now();
        
        if (isSpeaking) {
          // ç™ºè©±ä¸­
          if (speechStartTimeRef.current === null) {
            speechStartTimeRef.current = now;
            console.log('[VAD] Speech started');
          }
          
          // ç™ºè©±ä¸­ã®è¡¨ç¤ºï¼ˆãƒãƒƒãƒ•ã‚¡ãŒã‚ã‚Œã°ãã‚Œã‚’è¡¨ç¤ºï¼‰
          if (!isProcessingRef.current) {
            const activeBuffer = activeBufferRef.current === 'A' ? bufferARef : bufferBRef;
            if (activeBuffer.current) {
              const speechDuration = Math.floor((now - speechStartTimeRef.current) / 1000);
              setInterimTranscript(`ğŸ”Š ${activeBuffer.current} (${speechDuration}ç§’)`);
            } else {
              const speechDuration = Math.floor((now - speechStartTimeRef.current) / 1000);
              setInterimTranscript(`ğŸ”Š è´ã„ã¦ã„ã¾ã™... (${speechDuration}ç§’)`);
            }
          }
          
          silenceStartTimeRef.current = null;
          
          // VADã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’ã‚¯ãƒªã‚¢
          if (vadTimeoutRef.current) {
            clearTimeout(vadTimeoutRef.current);
            vadTimeoutRef.current = null;
          }
          
          // ãƒãƒƒãƒ•ã‚¡ç„¡éŸ³ã‚¿ã‚¤ãƒãƒ¼ã‚‚ã‚¯ãƒªã‚¢ï¼ˆç™ºè©±ä¸­ã¯Geminié€ä¿¡ã—ãªã„ï¼‰
          if (bufferSilenceTimerRef.current) {
            clearTimeout(bufferSilenceTimerRef.current);
            bufferSilenceTimerRef.current = null;
          }
          
          // æœ€å¤§ç™ºè©±æ™‚é–“ã‚’è¶…ãˆãŸã‚‰å¼·åˆ¶é€ä¿¡
          if (speechStartTimeRef.current && (now - speechStartTimeRef.current) > VAD_MAX_SPEECH_DURATION) {
            console.log('[VAD] Max speech duration reached, forcing send');
            processAudio();
            speechStartTimeRef.current = now; // ãƒªã‚»ãƒƒãƒˆã—ã¦ç¶™ç¶š
          }
        } else {
          // ç„¡éŸ³
          const activeBuffer = activeBufferRef.current === 'A' ? bufferARef : bufferBRef;
          if (speechStartTimeRef.current === null && !isProcessingRef.current) {
            // ã¾ã ç™ºè©±ãŒå§‹ã¾ã£ã¦ã„ãªã„
            if (activeBuffer.current) {
              setInterimTranscript(`ğŸ’¬ ${activeBuffer.current}`);
            } else {
              setInterimTranscript('ğŸ¤ éŸ³å£°ã‚’å¾…æ©Ÿä¸­...');
            }
          }
          if (speechStartTimeRef.current !== null) {
            // ç™ºè©±å¾Œã®ç„¡éŸ³
            if (silenceStartTimeRef.current === null) {
              silenceStartTimeRef.current = now;
            }
            
            const silenceDuration = now - silenceStartTimeRef.current;
            const speechDuration = now - speechStartTimeRef.current;
            
            // ç„¡éŸ³ä¸­ã®è¡¨ç¤º
            if (silenceDuration > 100 && !isProcessingRef.current) {
              if (activeBuffer.current) {
                setInterimTranscript(`â³ ${activeBuffer.current}...`);
              } else {
                setInterimTranscript(`â³ è¨€è‘‰ã®åŒºåˆ‡ã‚Šã‚’å¾…æ©Ÿä¸­... (${(silenceDuration/1000).toFixed(1)}ç§’)`);
              }
            }
            
            // ç„¡éŸ³ãŒä¸€å®šæ™‚é–“ç¶šã„ãŸã‚‰Whisperã«é€ä¿¡
            if (silenceDuration >= VAD_SILENCE_DURATION && speechDuration >= VAD_MIN_SPEECH_DURATION) {
              if (!isProcessingRef.current && !vadTimeoutRef.current) {
                vadTimeoutRef.current = setTimeout(() => {
                  console.log('[VAD] Silence detected after speech, sending audio');
                  processAudio();
                  speechStartTimeRef.current = null;
                  silenceStartTimeRef.current = null;
                  vadTimeoutRef.current = null;
                }, 50);
              }
            }
          }
        }
      });

      recorderRef.current = recorder;
      setState('listening');
      setProcessingStatus('è§£æä¸­');

    } catch (e) {
      console.error('[Whisper] Failed to start:', e);
      setError('ãƒã‚¤ã‚¯ã®ä½¿ç”¨ãŒè¨±å¯ã•ã‚Œã¦ã„ã¾ã›ã‚“');
      setState('idle');
      setProcessingStatus('');
    }
  }, [isSupported, currentGain, processAudio]);

  const stopListening = useCallback(async () => {
    setState('stopping');
    setProcessingStatus('åœæ­¢ä¸­...');

    // ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
    if (animationTimerRef.current) {
      clearTimeout(animationTimerRef.current);
      animationTimerRef.current = null;
    }
    if (bufferSilenceTimerRef.current) {
      clearTimeout(bufferSilenceTimerRef.current);
      bufferSilenceTimerRef.current = null;
    }
    if (vadTimeoutRef.current) {
      clearTimeout(vadTimeoutRef.current);
      vadTimeoutRef.current = null;
    }
    
    // ä¸¡æ–¹ã®ãƒãƒƒãƒ•ã‚¡ã«æ®‹ã‚ŠãŒã‚ã‚Œã°Geminiã«é€ä¿¡
    const remainingText = (bufferARef.current.trim() + ' ' + bufferBRef.current.trim()).trim();
    if (remainingText && onBufferReadyRef.current) {
      console.log('[Whisper] Flushing remaining buffers on stop:', remainingText);
      onBufferReadyRef.current(remainingText);
    }
    
    speechStartTimeRef.current = null;
    silenceStartTimeRef.current = null;
    bufferARef.current = '';
    bufferBRef.current = '';
    activeBufferRef.current = 'A';
    displayedTextRef.current = '';
    targetTextRef.current = '';

    // æœ€å¾Œã®éŸ³å£°ã‚’å‡¦ç†
    if (recorderRef.current) {
      const finalBlob = recorderRef.current.stop();
      
      // ç„¡éŸ³ã§ãªãã€ååˆ†ãªã‚µã‚¤ã‚ºãŒã‚ã‚‹å ´åˆã®ã¿å‡¦ç†
      if (finalBlob && finalBlob.size > 1000 && maxAudioLevelRef.current >= silenceThreshold) {
        setState('processing');
        setInterimTranscript('æœ€çµ‚å‡¦ç†ä¸­...');
        setProcessingStatus('æœ€çµ‚å‡¦ç†ä¸­...');
        
        try {
          const result = await transcribeAudio(finalBlob, whisperPromptRef.current);
          if (result.text && result.text.trim() && !isHallucination(result.text.trim())) {
            const finalText = result.text.trim();
            
            // ä¼šè©±æ¬„ã«è¿½åŠ 
            setTranscript((prev) => {
              return prev ? prev + '\n' + finalText : finalText;
            });
            
            // Geminiã«ã‚‚é€ä¿¡
            if (onBufferReadyRef.current) {
              onBufferReadyRef.current(finalText);
            }
          }
        } catch (e) {
          console.error('[Whisper] Final transcription error:', e);
        }
      }
      
      recorderRef.current = null;
    }

    setState('idle');
    setInterimTranscript('');
    setIsSpeechDetected(false);
    setAudioLevel(0);
    setProcessingStatus('');
    maxAudioLevelRef.current = 0;
    recentAudioLevelsRef.current = [];
  }, [silenceThreshold]);

  const clearTranscript = useCallback(() => {
    setTranscript('');
    setInterimTranscript('');
    pendingTextRef.current = '';
    bufferARef.current = '';
    bufferBRef.current = '';
    activeBufferRef.current = 'A';
    displayedTextRef.current = '';
    targetTextRef.current = '';
  }, []);

  // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
  useEffect(() => {
    return () => {
      if (vadTimeoutRef.current) {
        clearTimeout(vadTimeoutRef.current);
      }
      if (animationTimerRef.current) {
        clearTimeout(animationTimerRef.current);
      }
      if (bufferSilenceTimerRef.current) {
        clearTimeout(bufferSilenceTimerRef.current);
      }
      if (recorderRef.current) {
        recorderRef.current.stop();
      }
    };
  }, []);

  return {
    transcript,
    interimTranscript,
    state,
    isListening: state === 'listening' || state === 'processing',
    isSpeechDetected,
    isClipping,
    error,
    isSupported,
    audioLevel,
    currentGain,
    processingStatus,
    setGain,
    startListening,
    stopListening,
    clearTranscript,
  };
}

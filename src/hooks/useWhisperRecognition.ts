import { useState, useEffect, useRef, useCallback } from 'react';
import { AudioRecorder, transcribeAudio } from '../lib/whisper';

export type RecognitionState = 'idle' | 'starting' | 'listening' | 'processing' | 'stopping';

export interface UseWhisperRecognitionOptions {
  silenceThreshold?: number;
  whisperPrompt?: string;
  onBufferReady?: (text: string) => void;
}

// ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
const DEBUG = true;
const log = (category: string, ...args: unknown[]) => {
  if (DEBUG) {
    const time = new Date().toISOString().slice(11, 23);
    console.log(`[${time}][${category}]`, ...args);
  }
};

// å¹»è¦šãƒ•ãƒ¬ãƒ¼ã‚º
const HALLUCINATION_EXACT = [
  'ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ', 'ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™',
  'ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ', 'ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™',
  'ãŠç–²ã‚Œæ§˜ã§ã—ãŸ', 'ãŠã‚„ã™ã¿ãªã•ã„', 'ã•ã‚ˆã†ãªã‚‰',
  'Thank you for watching', 'Subscribe',
  '...', 'ã€‚ã€‚ã€‚', 'â€¦',
];

const HALLUCINATION_PARTIAL = [
  'ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²', 'ã”è¦–è´', 'è¦–è´', 'æ¬¡å›', 'æ¬¡ã®å‹•ç”»',
];

function isHallucination(text: string): boolean {
  const normalized = text.trim();
  const lower = normalized.toLowerCase();
  
  for (const phrase of HALLUCINATION_EXACT) {
    if (normalized === phrase || lower === phrase.toLowerCase()) return true;
  }
  for (const phrase of HALLUCINATION_PARTIAL) {
    if (lower.includes(phrase.toLowerCase())) return true;
  }
  if (normalized.length <= 4) return true;
  if (/^(.)\1{3,}$/.test(normalized)) return true;
  
  return false;
}

// ç”»é¢ã‚µã‚¤ã‚ºã‹ã‚‰è¡¨ç¤ºå¯èƒ½æ–‡å­—æ•°ã‚’è¨ˆç®—
function calculateMaxChars(): number {
  const realtimeElement = document.querySelector('.realtime-text');
  if (!realtimeElement) return 20;
  
  const computedStyle = window.getComputedStyle(realtimeElement);
  const width = realtimeElement.clientWidth;
  const paddingLeft = parseFloat(computedStyle.paddingLeft) || 0;
  const paddingRight = parseFloat(computedStyle.paddingRight) || 0;
  const availableWidth = width - paddingLeft - paddingRight - 30;
  const fontSize = parseFloat(computedStyle.fontSize) || 16;
  const maxChars = Math.floor((availableWidth / fontSize) * 0.8);
  
  return Math.max(10, maxChars);
}

export function useWhisperRecognition(options: UseWhisperRecognitionOptions = {}) {
  const {
    silenceThreshold = 0.05,
    whisperPrompt = '',
    onBufferReady,
  } = options;

  const [transcript, setTranscript] = useState<string>('');
  const [interimTranscript, setInterimTranscript] = useState<string>('');
  const [state, setState] = useState<RecognitionState>('idle');
  const [error, setError] = useState<string | null>(null);
  const [isSupported, setIsSupported] = useState<boolean>(false);
  const [isSpeechDetected, setIsSpeechDetected] = useState<boolean>(false);
  const [audioLevel, setAudioLevel] = useState<number>(0);
  const [isClipping, setIsClipping] = useState<boolean>(false);
  const [currentGain, setCurrentGain] = useState<number>(50);
  const [processingStatus, setProcessingStatus] = useState<string>('');

  const recorderRef = useRef<AudioRecorder | null>(null);
  
  // ===== ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”¨Whisperâ‘  =====
  const realtimeProcessingRef = useRef<boolean>(false);
  const displayTextRef = useRef<string>('');
  const maxCharsRef = useRef<number>(20);
  const realtimeIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);
  const realtimeAudioLevelRef = useRef<number>(0);
  const REALTIME_INTERVAL = 1500; // 1.5ç§’å›ºå®š
  
  // ===== ä¼šè©±ç”¨Whisperâ‘¡ =====
  const conversationProcessingRef = useRef<boolean>(false);
  const geminiBufferRef = useRef<string>('');
  const lastSpeechTimeRef = useRef<number>(Date.now());
  const vadTimerRef = useRef<ReturnType<typeof setTimeout> | null>(null);
  const VAD_SPEECH_THRESHOLD = 0.015;
  const VAD_SILENCE_DURATION = 400; // 0.4ç§’
  
  const whisperPromptRef = useRef<string>(whisperPrompt);
  const onBufferReadyRef = useRef(onBufferReady);

  useEffect(() => {
    onBufferReadyRef.current = onBufferReady;
  }, [onBufferReady]);

  useEffect(() => {
    whisperPromptRef.current = whisperPrompt;
  }, [whisperPrompt]);

  useEffect(() => {
    const supported = typeof navigator.mediaDevices !== 'undefined' && 
      typeof navigator.mediaDevices.getUserMedia === 'function';
    setIsSupported(supported);
    if (!supported) {
      setError('ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯éŸ³å£°éŒ²éŸ³ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚');
    }
  }, []);

  // ===== ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºæ›´æ–° =====
  const updateDisplay = useCallback((newText: string) => {
    const combined = displayTextRef.current + newText;
    const maxChars = maxCharsRef.current;
    
    if (combined.length > maxChars) {
      displayTextRef.current = combined.slice(-maxChars);
    } else {
      displayTextRef.current = combined;
    }
    
    log('REALTIME', `Display: "${displayTextRef.current}" (${displayTextRef.current.length}/${maxChars})`);
    setInterimTranscript(`ğŸ’¬ ${displayTextRef.current}`);
  }, []);

  // ===== Whisperâ‘ : ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”¨ï¼ˆ1.5ç§’å›ºå®šï¼‰ =====
  // è¡¨ç¤ºç”¨ãƒãƒƒãƒ•ã‚¡ã‚’å–å¾—ã—ã¦ã‚¯ãƒªã‚¢ï¼ˆä¼šè©±ç”¨ãƒãƒƒãƒ•ã‚¡ã¯åˆ¥ç®¡ç†ï¼‰
  const sendRealtimeWhisper = useCallback(async () => {
    if (!recorderRef.current || realtimeProcessingRef.current || !recorderRef.current.isRecording()) {
      return;
    }

    const maxLevel = realtimeAudioLevelRef.current;
    realtimeAudioLevelRef.current = 0;
    
    // è¡¨ç¤ºç”¨ãƒãƒƒãƒ•ã‚¡ã‚’å–å¾—ï¼ˆã‚¯ãƒªã‚¢ã•ã‚Œã‚‹ï¼‰
    const blob = recorderRef.current.getRealtimeBlob();
    
    // ãƒãƒ³ãƒ‰ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®éŸ³å£°ãƒ¬ãƒ™ãƒ«ã§åˆ¤å®šï¼ˆäººã®å£°ãŒãªã„å ´åˆã¯é€ä¿¡ã—ãªã„ï¼‰
    if (maxLevel < silenceThreshold) {
      log('REALTIME', `No voice detected (level: ${maxLevel.toFixed(3)}), skipping`);
      return;
    }

    // blobã¯æ—¢ã«å–å¾—æ¸ˆã¿
    
    if (!blob || blob.size < 1000) {
      return;
    }

    log('REALTIME', `Sending to Whisperâ‘ : ${blob.size} bytes, level: ${maxLevel.toFixed(3)}`);
    realtimeProcessingRef.current = true;

    try {
      const result = await transcribeAudio(blob, whisperPromptRef.current);
      
      if (result.text && result.text.trim() && !isHallucination(result.text.trim())) {
        const newText = result.text.trim();
        log('REALTIME', `Result: "${newText}"`);
        updateDisplay(newText);
      }
    } catch (e) {
      log('REALTIME', `Error: ${e}`);
    } finally {
      realtimeProcessingRef.current = false;
    }
  }, [silenceThreshold, updateDisplay]);

  // ===== Whisperâ‘¡: ä¼šè©±ç”¨ï¼ˆVAD 0.4ç§’ï¼‰ =====
  const sendConversationWhisper = useCallback(async (audioBlob: Blob) => {
    if (conversationProcessingRef.current) {
      log('CONVERSATION', 'Already processing, queuing...');
      return;
    }

    log('CONVERSATION', `Sending to Whisperâ‘¡: ${audioBlob.size} bytes`);
    conversationProcessingRef.current = true;
    setProcessingStatus('ä¼šè©±è§£æä¸­...');

    try {
      const result = await transcribeAudio(audioBlob, whisperPromptRef.current);
      
      if (result.text && result.text.trim() && !isHallucination(result.text.trim())) {
        const newText = result.text.trim();
        log('CONVERSATION', `Result: "${newText}"`);
        
        // Geminiãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
        geminiBufferRef.current = geminiBufferRef.current 
          ? geminiBufferRef.current + ' ' + newText 
          : newText;
        
        // transcriptæ›´æ–°
        setTranscript(prev => prev ? prev + '\n' + newText : newText);
        setProcessingStatus('èªè­˜æˆåŠŸ');
        
        // Geminiã«é€ä¿¡
        if (onBufferReadyRef.current && geminiBufferRef.current.trim()) {
          log('CONVERSATION', `Sending to Gemini: "${geminiBufferRef.current.slice(0, 50)}..."`);
          onBufferReadyRef.current(geminiBufferRef.current.trim());
          geminiBufferRef.current = '';
        }
      } else {
        log('CONVERSATION', 'No valid text');
        setProcessingStatus('éŸ³å£°ãªã—');
      }
    } catch (e) {
      log('CONVERSATION', `Error: ${e}`);
      setProcessingStatus('ã‚¨ãƒ©ãƒ¼');
    } finally {
      conversationProcessingRef.current = false;
    }
  }, []);

  // ===== VADå‡¦ç† =====
  const handleVAD = useCallback((level: number) => {
    const isSpeaking = level > VAD_SPEECH_THRESHOLD;
    setIsSpeechDetected(isSpeaking);
    
    if (isSpeaking) {
      // éŸ³å£°æ¤œå‡ºæ™‚ã®ãƒ­ã‚°ï¼ˆé »åº¦ã‚’æ¸›ã‚‰ã™ãŸã‚æ¡ä»¶ä»˜ãï¼‰
      if (!lastSpeechTimeRef.current || Date.now() - lastSpeechTimeRef.current > 1000) {
        log('VAD', `Speech detected, level: ${level.toFixed(3)}`);
      }
      lastSpeechTimeRef.current = Date.now();
      
      // VADã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢ï¼ˆè©±ã—ä¸­ï¼‰
      if (vadTimerRef.current) {
        clearTimeout(vadTimerRef.current);
        vadTimerRef.current = null;
      }
    } else {
      // ç„¡éŸ³ãŒç¶šã„ãŸã‚‰VADã‚¿ã‚¤ãƒãƒ¼é–‹å§‹
      const silenceDuration = Date.now() - lastSpeechTimeRef.current;
      
      // ãƒ‡ãƒãƒƒã‚°: ç„¡éŸ³æ™‚ã®çŠ¶æ…‹ã‚’ç¢ºèªï¼ˆé »åº¦ã‚’æ¸›ã‚‰ã™ï¼‰
      if (silenceDuration > 100 && silenceDuration < 600 && !vadTimerRef.current) {
        log('VAD', `Silence check: duration=${silenceDuration}ms, threshold=${VAD_SILENCE_DURATION}ms, timer=${!!vadTimerRef.current}, recorder=${!!recorderRef.current}`);
      }
      
      if (silenceDuration >= VAD_SILENCE_DURATION && !vadTimerRef.current && recorderRef.current) {
        log('VAD', `Silence duration: ${silenceDuration}ms, starting timer`);
        vadTimerRef.current = setTimeout(() => {
          vadTimerRef.current = null;
          
          // VADçµ‚äº†: ä¼šè©±ç”¨Whisperâ‘¡ã«é€ä¿¡
          if (recorderRef.current && recorderRef.current.isRecording()) {
            log('VAD', 'Getting conversation blob...');
            const blob = recorderRef.current.getConversationBlob();
            log('VAD', `Blob size: ${blob?.size || 0} bytes`);
            if (blob && blob.size > 1000) {
              log('VAD', `Silence detected (${silenceDuration}ms), sending to Whisperâ‘¡`);
              sendConversationWhisper(blob);
            } else {
              log('VAD', 'Blob too small or null, skipping');
            }
          } else {
            log('VAD', 'Recorder not recording, skipping');
          }
        }, 100); // å°‘ã—å¾…ã£ã¦ã‹ã‚‰é€ä¿¡
      }
    }
    
    // çŠ¶æ…‹è¡¨ç¤ºï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºãŒãªã„å ´åˆã®ã¿ï¼‰
    if (!displayTextRef.current) {
      if (isSpeaking) {
        setInterimTranscript('ğŸ”Š è´ã„ã¦ã„ã¾ã™...');
      } else {
        setInterimTranscript('ğŸ¤ éŸ³å£°ã‚’å¾…æ©Ÿä¸­...');
      }
    }
  }, [sendConversationWhisper]);

  const setGain = useCallback((value: number) => {
    setCurrentGain(value);
    if (recorderRef.current) {
      recorderRef.current.setGain(value);
    }
  }, []);

  const startListening = useCallback(async () => {
    if (!isSupported) {
      setError('éŸ³å£°éŒ²éŸ³ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“');
      return;
    }

    log('START', 'Starting dual Whisper listening...');
    setError(null);
    setState('starting');
    
    // å…¨ã¦ãƒªã‚»ãƒƒãƒˆ
    geminiBufferRef.current = '';
    displayTextRef.current = '';
    realtimeAudioLevelRef.current = 0;
    lastSpeechTimeRef.current = Date.now();
    
    // æœ€å¤§æ–‡å­—æ•°ã‚’è¨ˆç®—
    maxCharsRef.current = calculateMaxChars();
    log('START', `Max chars: ${maxCharsRef.current}`);

    try {
      const recorder = new AudioRecorder();
      recorder.setGain(currentGain);
      
      await recorder.start((level, clipping) => {
        setAudioLevel(level);
        setIsClipping(clipping);
        
        // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”¨ã®æœ€å¤§ãƒ¬ãƒ™ãƒ«ã‚’è¨˜éŒ²
        if (level > realtimeAudioLevelRef.current) {
          realtimeAudioLevelRef.current = level;
        }
        
        // VADå‡¦ç†
        handleVAD(level);
      });

      recorderRef.current = recorder;
      setState('listening');
      setProcessingStatus('è§£æä¸­');
      
      // ===== Whisperâ‘ : ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”¨ï¼ˆ1.5ç§’å›ºå®šï¼‰é–‹å§‹ =====
      realtimeIntervalRef.current = setInterval(() => {
        sendRealtimeWhisper();
      }, REALTIME_INTERVAL);
      
      log('START', 'Dual Whisper listening started');

    } catch (e) {
      log('START', `Error: ${e}`);
      setError('ãƒã‚¤ã‚¯ã®ä½¿ç”¨ãŒè¨±å¯ã•ã‚Œã¦ã„ã¾ã›ã‚“');
      setState('idle');
    }
  }, [isSupported, currentGain, sendRealtimeWhisper, handleVAD]);

  const stopListening = useCallback(async () => {
    log('STOP', 'Stopping...');
    setState('stopping');

    // ã‚¿ã‚¤ãƒãƒ¼ã‚¯ãƒªã‚¢
    if (realtimeIntervalRef.current) {
      clearInterval(realtimeIntervalRef.current);
      realtimeIntervalRef.current = null;
    }
    if (vadTimerRef.current) {
      clearTimeout(vadTimerRef.current);
      vadTimerRef.current = null;
    }
    
    // æ®‹ã‚Šãƒãƒƒãƒ•ã‚¡ã‚’Geminiã«é€ä¿¡
    if (geminiBufferRef.current.trim() && onBufferReadyRef.current) {
      log('STOP', `Sending remaining buffer: "${geminiBufferRef.current.slice(0, 50)}..."`);
      onBufferReadyRef.current(geminiBufferRef.current.trim());
    }
    
    geminiBufferRef.current = '';
    displayTextRef.current = '';

    if (recorderRef.current) {
      const finalBlob = recorderRef.current.stop();
      
      // æœ€çµ‚éŸ³å£°ãŒã‚ã‚Œã°ä¼šè©±ç”¨Whisperâ‘¡ã«é€ä¿¡
      if (finalBlob && finalBlob.size > 1000) {
        setState('processing');
        setInterimTranscript('æœ€çµ‚å‡¦ç†ä¸­...');
        
        try {
          const result = await transcribeAudio(finalBlob, whisperPromptRef.current);
          if (result.text && result.text.trim() && !isHallucination(result.text.trim())) {
            const finalText = result.text.trim();
            setTranscript(prev => prev ? prev + '\n' + finalText : finalText);
            if (onBufferReadyRef.current) {
              onBufferReadyRef.current(finalText);
            }
          }
        } catch (e) {
          log('STOP', `Final error: ${e}`);
        }
      }
      
      recorderRef.current = null;
    }

    setState('idle');
    setInterimTranscript('');
    setIsSpeechDetected(false);
    setAudioLevel(0);
    setProcessingStatus('');
    log('STOP', 'Stopped');
  }, []);

  const clearTranscript = useCallback(() => {
    setTranscript('');
    setInterimTranscript('');
    geminiBufferRef.current = '';
    displayTextRef.current = '';
  }, []);

  useEffect(() => {
    return () => {
      if (realtimeIntervalRef.current) clearInterval(realtimeIntervalRef.current);
      if (vadTimerRef.current) clearTimeout(vadTimerRef.current);
      if (recorderRef.current) recorderRef.current.stop();
    };
  }, []);

  return {
    transcript,
    interimTranscript,
    state,
    isListening: state === 'listening' || state === 'processing',
    isSpeechDetected,
    isClipping,
    error,
    isSupported,
    audioLevel,
    currentGain,
    processingStatus,
    setGain,
    startListening,
    stopListening,
    clearTranscript,
  };
}
